<div id="badges">
  <a href="https://www.linkedin.com/in/vasilev-vitalii/">
    <img src="https://img.shields.io/badge/LinkedIn-blue?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn Badge"/>
  </a>
  <a href="https://www.youtube.com/@user-gj9vk5ln5c/featured">
    <img src="https://img.shields.io/badge/YouTube-red?style=for-the-badge&logo=youtube&logoColor=white" alt="Youtube Badge"/>
  </a>
</div>

[English](README.md)

# mentator-llm-prompter

Мощная CLI-утилита для пакетной обработки промптов через различные AI-провайдеры с поддержкой нескольких режимов работы.

## Возможности

- **Поддержка множества AI-провайдеров**: Работает с OpenAI-совместимыми API, Ollama и Mentator LLM Service
- **Три режима работы**: Basic, Template и JSON Pipeline для различных сценариев использования
- **Пакетная обработка**: Автоматическая обработка множества файлов со встроенным кэшированием
- **Структурированный вывод**: Поддержка JSON-схем с принудительным соблюдением грамматики
- **Отслеживание прогресса**: Индикаторы прогресса в реальном времени и подробная статистика
- **JSONC конфигурация**: Удобный формат конфигурации с поддержкой комментариев

## Установка

```bash
npm install
npm run build
```

## Быстрый старт

1. Сгенерировать шаблон конфигурации:
```bash
node dist/index.js --conf-gen /путь/к/директории
```

2. Отредактировать сгенерированный файл `mentator-llm-prompter.config.TEMPLATE.jsonc`

3. Сгенерировать шаблон промпта (опционально):
```bash
# Для текстовых ответов
node dist/index.js --prompt-gen /путь/к/директории

# Для JSON-ответов
node dist/index.js --prompt-gen-json /путь/к/директории
```

4. Запустить обработку:
```bash
node dist/index.js --conf-use /путь/к/вашему/config.jsonc
```

## Конфигурация

Файл конфигурации использует формат JSONC (JSON с комментариями). Структура:

```jsonc
{
  "log": {
    "dir": "/путь/к/логам",
    "mode": "REWRITE"  // или "APPEND"
  },
  "ai": {
    "kind": "openapi",  // или "ollama" или "mentator"
    "url": "http://localhost:11434",
    "api_key": "ваш-api-ключ",  // опционально для Ollama, обязательно для OpenAI
    "timeout": 600000,
    "model": "deepseek-coder:6.7b"
  },
  "prompt": {
    "dir": "/путь/к/входным/файлам",
    "templateReplacePayload": "{{payload}}",
    "templateReplaceJson": "{{json}}",
    "templateFile": [
      "/путь/к/шаблону1.txt",
      "/путь/к/шаблону2.txt"
    ]
  },
  "answer": {
    "dir": "/путь/к/выходной/папке",
    "hashDir": "/путь/к/хэшам"  // опционально: для отслеживания изменений
  }
}
```

## Режимы работы

Приложение автоматически выбирает режим работы на основе конфигурации. Каждый режим служит для разных задач.

### Режим 1: Basic (Простая обработка)

**Когда активируется**: Не указаны файлы шаблонов (`templateFile` пуст или не задан)

**Как работает**:
- Каждый файл в `prompt.dir` рассматривается как полноценный промпт
- Содержимое файла отправляется напрямую в AI как пользовательское сообщение
- Один ответ на один файл
- Идеально для простых, самостоятельных вопросов

**Пример использования**:
```
prompt.dir/
├── вопрос1.txt ("Что такое TypeScript?")
├── вопрос2.txt ("Объясни async/await")
└── вопрос3.txt ("Что такое замыкания?")

→ Каждый файл генерирует один файл ответа с тем же именем
```

**Конфигурация**:
```jsonc
{
  "prompt": {
    "dir": "/путь/к/вопросам",
    // templateFile не указан
  }
}
```

### Режим 2: Template (Множественные промпты на файл)

**Когда активируется**: Указаны файлы шаблонов И не требуется JSON-ответ

**Как работает**:
- Использует предопределенные шаблоны промптов с плейсхолдерами `{{payload}}`
- Каждый входной файл обрабатывается ВСЕМИ шаблонами
- Генерирует несколько файлов ответов на один входной файл (по одному на шаблон)
- Шаблоны могут иметь системные сообщения и настройки
- Идеально для анализа одного контента с разных перспектив

**Пример использования**:
```
Анализ файлов кода с разных сторон:

шаблон1.txt:
---
system: Вы эксперт по безопасности
user: Проверьте этот код на уязвимости: {{payload}}
---

шаблон2.txt:
---
system: Вы эксперт по производительности
user: Проанализируйте производительность: {{payload}}
---

Вход: code.js
Выход:
  - code.js/answer-000-000.txt (проверка безопасности)
  - code.js/answer-001-000.txt (анализ производительности)
```

**Формат шаблона** (используя vv-ai-prompt-format):
```
---
system: Системное сообщение
user: Сообщение пользователя с плейсхолдером {{payload}}
options:
  temperature: 0.7
  max_tokens: 1000
---
```

**Конфигурация**:
```jsonc
{
  "prompt": {
    "dir": "/путь/к/файлам",
    "templateReplacePayload": "{{payload}}",
    "templateFile": [
      "/путь/к/проверка-безопасности.txt",
      "/путь/к/анализ-производительности.txt"
    ]
  }
}
```

### Режим 3: JSON Pipeline (Цепочка структурированных запросов)

**Когда активируется**:
- AI-провайдер - `mentator`
- В шаблонах определен `jsonresponse` (JSON-схема)
- Все шаблоны должны единообразно использовать или не использовать `jsonresponse`

**Как работает**:
- Обрабатывает вход через цепочку промптов
- Каждый промпт должен вернуть JSON, соответствующий схеме
- Результат промпта N становится переменной `{{json}}` для промпта N+1
- Несколько промптов в одном файле шаблона = варианты одного вопроса (пробует до непустого результата)
- Разные файлы шаблонов = последовательные вопросы в пайплайне
- Постепенно накапливает и обогащает результат
- **Особое правило**: Если первый вопрос вернул пустой результат - сохраняет пустой и останавливается (успех)
- **Особое правило**: Если последующие вопросы вернули пустой результат - ошибка

**Пример использования**:
```
Извлечение структурированных данных через несколько этапов:

Шаг 1 (шаблон1.txt - извлечь базовую информацию):
---
user: Извлеки всех людей из этого текста: {{payload}}
jsonresponse:
{
  "type": "array",
  "items": {
    "type": "object",
    "properties": {
      "name": {"type": "string"},
      "age": {"type": "integer"}
    }
  }
}
---

Шаг 2 (шаблон2.txt - обогатить дополнительными данными):
---
user: Добавь профессии для этих людей: {{json}}
Контекст: {{payload}}
jsonresponse:
{
  "type": "array",
  "items": {
    "type": "object",
    "properties": {
      "name": {"type": "string"},
      "age": {"type": "integer"},
      "occupation": {"type": "string"}
    }
  }
}
---

Результат: Один обогащенный JSON-файл с полной информацией
```

**Вариантные вопросы** (несколько промптов в одном файле):
```
Все промпты в ОДНОМ файле шаблона - это варианты одного вопроса.
Система пробует их последовательно, пока один не вернет непустой JSON.

Пример - пробуем разные стратегии извлечения:
---
user: Извлеки людей, используя формальный язык: {{payload}}
jsonresponse: {...}
---
user: Извлеки людей, используя простой язык: {{payload}}
jsonresponse: {...}
---
user: Перечисли все упоминания персон: {{payload}}
jsonresponse: {...}
---
```

**Конфигурация**:
```jsonc
{
  "ai": {
    "kind": "mentator",  // Обязательно для режима JSON Pipeline
    "url": "http://localhost:12345"
  },
  "prompt": {
    "dir": "/путь/к/документам",
    "templateReplacePayload": "{{payload}}",
    "templateReplaceJson": "{{json}}",
    "templateFile": [
      "/путь/к/шаг1-извлечение.txt",
      "/путь/к/шаг2-обогащение.txt",
      "/путь/к/шаг3-валидация.txt"
    ]
  }
}
```

**Шаблон с JSON-ответом**:
```
---
system: Извлекай структурированные данные
user: Получи всех людей из: {{payload}}
options:
  temperature: 0
segment:
  mentator-llm-service: '{"useGrammar":true}'
jsonresponse:
{
  "type": "array",
  "items": {
    "type": "object",
    "properties": {
      "name": {"type": "string"},
      "age": {"type": "integer"}
    },
    "required": ["name"]
  }
}
---
```

## Сравнительная таблица режимов

| Характеристика | Basic | Template | JSON Pipeline |
|----------------|-------|----------|---------------|
| **Конфигурация** | Без шаблонов | Шаблоны без JSON | Шаблоны с JSON + Mentator |
| **Вход → Выход** | 1 → 1 | 1 → N | 1 → 1 (обогащенный) |
| **Применение** | Простые вопросы | Многосторонний анализ | Пайплайн извлечения данных |
| **Цепочка** | Нет | Нет | Да ({{json}}) |
| **Структурированный вывод** | Опционально | Опционально | Обязательно |
| **Поддержка вариантов** | Нет | Нет | Да (в одном шаблоне) |
| **Обработка пустого результата** | Ошибка | Ошибка | Первый: ОК, Остальные: Ошибка |

## Конфигурация AI-провайдеров

### OpenAI (и совместимые API)

```jsonc
{
  "ai": {
    "kind": "openapi",
    "url": "https://api.openai.com",
    "api_key": "sk-...",
    "model": "gpt-4",
    "timeout": 600000
  }
}
```

### Ollama

```jsonc
{
  "ai": {
    "kind": "ollama",
    "url": "http://localhost:11434",
    "model": "llama2:13b",
    "timeout": 600000
  }
}
```

### Mentator LLM Service

```jsonc
{
  "ai": {
    "kind": "mentator",
    "url": "http://localhost:19777",
    "model": "имя-модели.gguf",
    "timeout": 600000
  }
}
```

## Отслеживание изменений на основе хэша

Включите умное кэширование для пропуска неизменных файлов:

```jsonc
{
  "answer": {
    "dir": "/путь/к/ответам",
    "hashDir": "/путь/к/хэшам"  // Хранить SHA-256 хэши здесь
  }
}
```

При включении:
- Вычисляет SHA-256 хэш каждого входного файла
- Сравнивает с сохраненным хэшем
- Пропускает обработку, если содержимое не изменилось
- Экономит время и расходы на API

## Вывод и логирование

### Консольный вывод
- Прогресс в реальном времени с процентами: `(25%) answer saved for "file.txt"`
- Итоговая статистика: `FILES STATISTICS: total=100, success=95, skipped=3, error=2`

### Файлы логов
- **Режим REWRITE**: Один файл `mentator-llm-prompter.log` (перезаписывается при каждом запуске)
- **Режим APPEND**: Логи с временными метками, например `mentator-llm-prompter.20240115-143022.log`

### Файлы ответов
- **Режим Basic**: `answer/file.txt`
- **Режим Template**: `answer/file.txt/answer-000-000.txt`, `answer/000-001.txt`, ...
- **Режим JSON Pipeline**: `answer/file.txt` (формат JSON)

## Справка по командам

```bash
# Сгенерировать шаблон конфигурации
--conf-gen /путь/к/директории

# Сгенерировать шаблон промпта (обычный текст)
--prompt-gen /путь/к/директории

# Сгенерировать шаблон промпта (JSON режим)
--prompt-gen-json /путь/к/директории

# Запустить обработку
--conf-use /путь/к/config.jsonc

# Показать справку
# (без аргументов)
```

## Примеры использования

### Пример 1: Код-ревью (режим Template)
Проверка нескольких файлов кода с разных сторон:
- Создать шаблоны для: безопасность, производительность, лучшие практики, документация
- Обработать все `.js` файлы в репозитории
- Получить 4 файла обзора на каждый исходный файл

### Пример 2: Анализ документов (режим Basic)
Анализ коллекции документов:
- Один вопрос на документ в отдельных файлах
- Пакетная обработка
- Каждый документ получает один ответ

### Пример 3: Извлечение структурированных данных (режим JSON Pipeline)
Извлечение и обогащение данных из неструктурированного текста:
- Шаг 1: Извлечь сущности (люди, места, даты)
- Шаг 2: Добавить связи
- Шаг 3: Валидировать и нормализовать
- Получить один обогащенный JSON на документ

### Пример 4: Пайплайн перевода контента (режим JSON Pipeline)
- Шаг 1: Извлечь ключевые фразы
- Шаг 2: Перевести с сохранением контекста
- Шаг 3: Проверить согласованность терминологии
- Использует предыдущие результаты на каждом шаге

## Решение проблем

### Шаблоны не активируются
- Проверьте, что массив `templateFile` не пуст
- Убедитесь, что пути к файлам абсолютные
- Проверьте существование и читаемость файлов

### JSON Pipeline не работает
- Убедитесь, что `ai.kind` установлен в `"mentator"`
- Проверьте, что во всех шаблонах определен `jsonresponse`
- Валидируйте синтаксис JSON-схемы
- Не смешивайте шаблоны с JSON-ответом и без

### Пустые результаты в JSON Pipeline
- Первый вопрос вернул пустое: Это нормально, результат сохранен как успех
- Последующие вопросы пустые: Смотрите логи ошибок, это указывает на проблему
- Попробуйте использовать вариантные вопросы (несколько промптов в одном шаблоне)

### Проблемы производительности
- Включите `hashDir` для пропуска неизмененных файлов
- Уменьшите `timeout` для более быстрого обнаружения сбоев
- Обрабатывайте меньшие пакеты
- Проверьте время ответа AI-провайдера

## Зависимости

- **vv-config-jsonc**: Обработка JSONC конфигурации
- **vv-ai-prompt-format**: Парсинг шаблонов промптов
- **@sinclair/typebox**: Валидация схем
- **minimist**: Парсинг аргументов командной строки

## Лицензия

См. файл LICENSE для деталей.

## Ссылки

- GitHub: https://github.com/VasilevVitalii/mentator-prompter
- Mentator LLM Service: [ссылка на сервис mentator]
- Проблемы: https://github.com/VasilevVitalii/mentator-prompter/issues
